{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word2Vec from Scratch and Transformer Implementation\n",
        "\n",
        "In this notebook, I build a Word2Vec model from scratch ,\n",
        "I relied heavily on ChatGPT to write most of the functions and explanations as I found limited detailed resources to implement word2vec from scratch, so using ChatGPT helped me understand and code these concepts clearly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "I5oqJK5DvV9W",
        "outputId": "583f0bca-705f-4136-9f8b-6549c516ecc5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Accountant</td>\n",
              "      <td>education omba executive leadership university...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Accountant</td>\n",
              "      <td>howard gerrard accountant deyjobcom birmingham...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Accountant</td>\n",
              "      <td>kevin frank senior accountant inforesumekraftc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Accountant</td>\n",
              "      <td>place birth nationality olivia ogilvy accounta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Accountant</td>\n",
              "      <td>stephen greet cpa senior accountant 9 year exp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13384</th>\n",
              "      <td>Web Designing</td>\n",
              "      <td>jessica claire montgomery street san francisco...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13385</th>\n",
              "      <td>Web Designing</td>\n",
              "      <td>jessica claire montgomery street san francisco...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13386</th>\n",
              "      <td>Web Designing</td>\n",
              "      <td>summary jessica claire 100 montgomery st 10th ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13387</th>\n",
              "      <td>Web Designing</td>\n",
              "      <td>jessica claire montgomery street san francisco...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13388</th>\n",
              "      <td>Web Designing</td>\n",
              "      <td>websites portfolios profiles professional summ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13389 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Category                                               Text\n",
              "0         Accountant  education omba executive leadership university...\n",
              "1         Accountant  howard gerrard accountant deyjobcom birmingham...\n",
              "2         Accountant  kevin frank senior accountant inforesumekraftc...\n",
              "3         Accountant  place birth nationality olivia ogilvy accounta...\n",
              "4         Accountant  stephen greet cpa senior accountant 9 year exp...\n",
              "...              ...                                                ...\n",
              "13384  Web Designing  jessica claire montgomery street san francisco...\n",
              "13385  Web Designing  jessica claire montgomery street san francisco...\n",
              "13386  Web Designing  summary jessica claire 100 montgomery st 10th ...\n",
              "13387  Web Designing  jessica claire montgomery street san francisco...\n",
              "13388  Web Designing  websites portfolios profiles professional summ...\n",
              "\n",
              "[13389 rows x 2 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oiKBwcP9wI4w",
        "outputId": "3a119bf2-9b13-4b11-8069-d9c347284ef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13389, 2)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Category\n",
              "Education                    410\n",
              "Electrical Engineering       384\n",
              "Mechanical Engineer          384\n",
              "Consultant                   368\n",
              "Sales                        364\n",
              "Civil Engineer               364\n",
              "Management                   361\n",
              "Human Resources              360\n",
              "Digital Media                358\n",
              "Accountant                   350\n",
              "Java Developer               348\n",
              "Operations Manager           345\n",
              "Building and Construction    345\n",
              "Testing                      344\n",
              "Architecture                 344\n",
              "Aviation                     340\n",
              "Business Analyst             340\n",
              "Finance                      339\n",
              "SQL Developer                338\n",
              "Public Relations             337\n",
              "Health and Fitness           332\n",
              "Arts                         332\n",
              "Network Security Engineer    330\n",
              "DotNet Developer             329\n",
              "Apparel                      320\n",
              "Banking                      314\n",
              "Automobile                   313\n",
              "Web Designing                309\n",
              "SAP Developer                304\n",
              "Data Science                 299\n",
              "ETL Developer                294\n",
              "Agriculture                  293\n",
              "Advocate                     291\n",
              "DevOps                       289\n",
              "PMO                          286\n",
              "Information Technology       274\n",
              "Designing                    258\n",
              "Database                     257\n",
              "Python Developer             248\n",
              "BPO                          203\n",
              "React Developer              182\n",
              "Food and Beverages           162\n",
              "Blockchain                    47\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df.shape)\n",
        "df['Category'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP-lR3p0wOjZ",
        "outputId": "b1afa3d4-e042-4895-bb37-dc8cbcb1aa6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # remove special characters and punctuations\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    text =  ' '.join(words)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdpaD-U1yOok",
        "outputId": "5b3bebf9-e501-4b67-c6ce-3c7186dd190a"
      },
      "outputs": [],
      "source": [
        "df['Text'] = df['Text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizing Text by Splitting Words\n",
        "\n",
        "creating a new column `'tokens'` in the DataFrame by splitting each sentence in the `'Text'` column into a list of words using the simple `split()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UOF14Tn5yeyv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                Text  \\\n",
            "0  education omba executive leadership university...   \n",
            "1  howard gerrard accountant deyjobcom birmingham...   \n",
            "2  kevin frank senior accountant inforesumekraftc...   \n",
            "3  place birth nationality olivia ogilvy accounta...   \n",
            "4  stephen greet cpa senior accountant 9 year exp...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [education, omba, executive, leadership, unive...  \n",
            "1  [howard, gerrard, accountant, deyjobcom, birmi...  \n",
            "2  [kevin, frank, senior, accountant, inforesumek...  \n",
            "3  [place, birth, nationality, olivia, ogilvy, ac...  \n",
            "4  [stephen, greet, cpa, senior, accountant, 9, y...  \n"
          ]
        }
      ],
      "source": [
        "df['tokens'] = df['Text'].apply(lambda x: x.split())\n",
        "print(df[['Text', 'tokens']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building Vocabulary from Tokenized Text\n",
        "\n",
        "- Combine all token lists into one big list of words  \n",
        "- Count how often each word appears using `Counter`  \n",
        "- Create a vocabulary dictionary that maps each unique word to a unique index based on frequency  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6ZCDh9qzHbf",
        "outputId": "2d022c11-d687-426f-9eba-931c347e292e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 121352\n",
            "Sample vocab items: [('management', 0), ('customer', 1), ('project', 2), ('data', 3), ('team', 4), ('service', 5), ('experience', 6), ('system', 7), ('skill', 8), ('business', 9)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten the list of token lists into one big list\n",
        "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
        "\n",
        "# Count frequency of each word\n",
        "word_counts = Counter(all_tokens)\n",
        "\n",
        "# Create vocab: word → unique index\n",
        "vocab = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Sample vocab items: {list(vocab.items())[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Context Pairs with Sliding Window\n",
        "\n",
        "Function generates pairs of words where each center word is paired with its surrounding context words within a given window size (2 here). For every token list in the dataset, we apply this sliding window to collect all word pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AHDSnFevzvKz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training pairs: 25356034\n",
            "Example pairs: [('education', 'omba'), ('education', 'executive'), ('omba', 'education'), ('omba', 'executive'), ('omba', 'leadership'), ('executive', 'education'), ('executive', 'omba'), ('executive', 'leadership'), ('executive', 'university'), ('leadership', 'omba')]\n"
          ]
        }
      ],
      "source": [
        "def sliding_window(tokens, window_size=2):\n",
        "    pairs = []\n",
        "    for i, center_word in enumerate(tokens):\n",
        "        context_indices = list(range(max(0, i - window_size), i)) + \\\n",
        "                          list(range(i + 1, min(len(tokens), i + window_size + 1)))\n",
        "        for j in context_indices:\n",
        "            pairs.append((center_word, tokens[j]))\n",
        "    return pairs\n",
        "\n",
        "# Generate pairs for the entire dataset\n",
        "all_pairs = []\n",
        "for tokens in df['tokens']:\n",
        "    all_pairs.extend(sliding_window(tokens, window_size=2))\n",
        "\n",
        "print(f\"Total training pairs: {len(all_pairs)}\")\n",
        "print(f\"Example pairs: {all_pairs[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Converting Word Pairs to Index Pairs\n",
        "\n",
        "Covertung the word pairs into pairs of their corresponding indices from the vocabulary also checking if both words are in the vocabulary and then creating a list of indexed pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaOScd8BBKCX",
        "outputId": "04565fe4-9c7f-4616-9e48-5291245f0a18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of indexed pairs: 25356034\n",
            "Sample indexed pairs: [(24, 25928), (24, 357), (25928, 24), (25928, 357), (25928, 121), (357, 24), (357, 25928), (357, 121), (357, 44), (121, 25928)]\n"
          ]
        }
      ],
      "source": [
        "def pairs_to_indices(pairs, vocab):\n",
        "    pairs_idx = []\n",
        "    for center_word, context_word in pairs:\n",
        "        if center_word in vocab and context_word in vocab:\n",
        "            pairs_idx.append((vocab[center_word], vocab[context_word]))\n",
        "    return pairs_idx\n",
        "\n",
        "training_pairs_idx = pairs_to_indices(all_pairs, vocab)\n",
        "\n",
        "print(f\"Number of indexed pairs: {len(training_pairs_idx)}\")\n",
        "print(f\"Sample indexed pairs: {training_pairs_idx[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Dataset and DataLoader for Word2Vec Training\n",
        "\n",
        "Here, created a custom PyTorch `Dataset` class that takes the list of word index pairs. The class implements the necessary methods to get the length and individual pairs as tensors.\n",
        "\n",
        "Then,  made a `DataLoader` to load the data in batches of 1024 and shuffle it every epoch to help the model learn better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5rAv96U0peW",
        "outputId": "7f95ee4a-91d9-498f-fa90-a722b31d1a48"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Word2VecDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        center, context = self.pairs[idx]\n",
        "        return torch.tensor(center, dtype=torch.long), torch.tensor(context, dtype=torch.long)\n",
        "\n",
        "dataset = Word2VecDataset(training_pairs_idx)\n",
        "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Word2Vec Model with Negative Sampling\n",
        "\n",
        "Defined the Word2Vec model class here. It has two embedding layers — one for the center words and one for the context words.\n",
        "\n",
        "In the forward method:\n",
        "- get embeddings for the center words, positive context words, and negative context words.\n",
        "- Then calculate scores for positive pairs using dot products and apply `logsigmoid` to get the positive loss.\n",
        "- For negative samples, calculate dot products (with a negation), apply `logsigmoid`, and sum for the negative loss.\n",
        "- Finally, combine these losses and return the average loss for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmPGJjgr1D5F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.input_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.output_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, center_words, pos_context_words, neg_context_words):\n",
        "\n",
        "        center_embeds = self.input_embeddings(center_words)               \n",
        "        pos_embeds = self.output_embeddings(pos_context_words)            \n",
        "        neg_embeds = self.output_embeddings(neg_context_words)             \n",
        "\n",
        "        # Positive score: dot(center, pos_context)\n",
        "        pos_score = torch.sum(center_embeds * pos_embeds, dim=1)           \n",
        "        pos_loss = F.logsigmoid(pos_score)\n",
        "\n",
        "        # Negative score: dot(center, neg_context)\n",
        "        neg_score = torch.bmm(neg_embeds.neg(), center_embeds.unsqueeze(2)).squeeze()   \n",
        "        neg_loss = F.logsigmoid(neg_score).sum(1)                      \n",
        "\n",
        "        loss = -(pos_loss + neg_loss).mean()\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Negative Sampling Distribution and Function\n",
        "\n",
        "Here, function first calculate the frequency of each word in the vocabulary and smooth it by raising the frequencies to the power of 0.75, which is a common trick in Word2Vec to balance frequent and rare words.\n",
        "\n",
        "Then, normalized the frequencies so they sum to 1.\n",
        "\n",
        "Finally, define a function `get_negative_samples` that randomly picks negative samples for each batch according to this smoothed frequency distribution. This helps the model learn by contrasting real context words with these negative samples during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN5sxWE41hm6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate word frequencies (from your vocab)\n",
        "word_counts_array = np.array([word_counts[word] for word, _ in sorted(vocab.items(), key=lambda x: x[1])])\n",
        "word_freqs = word_counts_array / word_counts_array.sum()\n",
        "\n",
        "# Apply smoothing (power 3/4)\n",
        "word_freqs = word_freqs ** 0.75\n",
        "word_freqs = word_freqs / word_freqs.sum()\n",
        "\n",
        "def get_negative_samples(batch_size, neg_samples):\n",
        "    neg_samples_idx = np.random.choice(len(vocab), size=(batch_size, neg_samples), p=word_freqs)\n",
        "    return torch.LongTensor(neg_samples_idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "RX6olM4R1y9j",
        "outputId": "1aa54f68-32ef-43d0-8dc6-d6bef3913573"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|██████████| 24762/24762 [1:07:49<00:00,  6.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 finished. Average Loss: 6.6390\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/3: 100%|██████████| 24762/24762 [1:03:33<00:00,  6.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 finished. Average Loss: 2.5436\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/3: 100%|██████████| 24762/24762 [1:00:43<00:00,  6.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 finished. Average Loss: 2.2090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "embedding_dim = 100\n",
        "neg_samples = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Word2Vec(len(vocab), embedding_dim).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for center_words, pos_context_words in loop:\n",
        "        center_words = center_words.to(device)\n",
        "        pos_context_words = pos_context_words.to(device)\n",
        "\n",
        "        batch_size = center_words.size(0)\n",
        "        neg_context_words = get_negative_samples(batch_size, neg_samples).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(center_words, pos_context_words, neg_context_words)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Word embeddings extracted successfully. Shape: torch.Size([121352, 100])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_33536\\3889904269.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('word2vec_checkpoint.pth'))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "embedding_dim = 100\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "model = Word2Vec(vocab_size, embedding_dim)\n",
        "\n",
        "model.load_state_dict(torch.load('word2vec_checkpoint.pth'))\n",
        "model.eval()\n",
        "\n",
        "word_embeddings = model.input_embeddings.weight.data.cpu()\n",
        "\n",
        "print(\"✅ Word embeddings extracted successfully. Shape:\", word_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Finding Similar Words Using Cosine Similarity\n",
        "`find_similar_words` takes a query word and finds the top N most similar words based on cosine similarity between embedding vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "embeddings = model.input_embeddings.weight.data.cpu().numpy()\n",
        "\n",
        "# Function to find top-N similar words by cosine similarity\n",
        "def find_similar_words(query_word, embeddings, word_id, id_word, top_n=10):\n",
        "    if query_word not in word_id:\n",
        "        print(f\"'{query_word}' not in vocabulary.\")\n",
        "        return\n",
        "    \n",
        "    query_idx = word_id[query_word]\n",
        "    query_vec = embeddings[query_idx].reshape(1, -1)\n",
        "    \n",
        "    cos_sim = cosine_similarity(query_vec, embeddings)[0]\n",
        "    top_ids = cos_sim.argsort()[-top_n-1:][::-1]  # top_n+1 because query word will be most similar to itself\n",
        "    \n",
        "    print(f\"Top {top_n} words similar to '{query_word}':\")\n",
        "    for idx in top_ids[1:]:  # skip the first, which is the query word itself\n",
        "        print(f\"{id_word[idx]} (score: {cos_sim[idx]:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 words similar to 'web':\n",
            "restful (score: 0.8151)\n",
            "api (score: 0.7866)\n",
            "soap (score: 0.7685)\n",
            "ui (score: 0.7654)\n",
            "application (score: 0.7514)\n",
            "apis (score: 0.7450)\n",
            "rest (score: 0.7418)\n",
            "frontend (score: 0.7416)\n",
            "html (score: 0.7237)\n",
            "java (score: 0.7146)\n"
          ]
        }
      ],
      "source": [
        "word_id = vocab\n",
        "id_word = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "find_similar_words(\"web\", embeddings, word_id, id_word, top_n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings_np = word_embeddings.numpy()\n",
        "np.save(\"word2vec_embeddings.npy\", embeddings_np)  # Save for reuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Label Encoding Mapping:\n",
            "Accountant → 0\n",
            "Advocate → 1\n",
            "Agriculture → 2\n",
            "Apparel → 3\n",
            "Architecture → 4\n",
            "Arts → 5\n",
            "Automobile → 6\n",
            "Aviation → 7\n",
            "BPO → 8\n",
            "Banking → 9\n",
            "Blockchain → 10\n",
            "Building and Construction → 11\n",
            "Business Analyst → 12\n",
            "Civil Engineer → 13\n",
            "Consultant → 14\n",
            "Data Science → 15\n",
            "Database → 16\n",
            "Designing → 17\n",
            "DevOps → 18\n",
            "Digital Media → 19\n",
            "DotNet Developer → 20\n",
            "ETL Developer → 21\n",
            "Education → 22\n",
            "Electrical Engineering → 23\n",
            "Finance → 24\n",
            "Food and Beverages → 25\n",
            "Health and Fitness → 26\n",
            "Human Resources → 27\n",
            "Information Technology → 28\n",
            "Java Developer → 29\n",
            "Management → 30\n",
            "Mechanical Engineer → 31\n",
            "Network Security Engineer → 32\n",
            "Operations Manager → 33\n",
            "PMO → 34\n",
            "Public Relations → 35\n",
            "Python Developer → 36\n",
            "React Developer → 37\n",
            "SAP Developer → 38\n",
            "SQL Developer → 39\n",
            "Sales → 40\n",
            "Testing → 41\n",
            "Web Designing → 42\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "df['label'] = label_encoder.fit_transform(df['Category'])\n",
        "\n",
        "print(\"\\nLabel Encoding Mapping:\")\n",
        "for i, category in enumerate(label_encoder.classes_):\n",
        "    print(f\"{category} → {i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing transformer style classifier from here on similar to what in previous notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training set shape: (10711, 4)\n",
            "Validation set shape: (2678, 4)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data (80% train, 20% validation)\n",
        "train_df, val_df = train_test_split(df[['Category', 'Text', 'tokens', 'label']], test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shapes\n",
        "print(f\"\\nTraining set shape: {train_df.shape}\")\n",
        "print(f\"Validation set shape: {val_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_texts = train_df[\"tokens\"].tolist()\n",
        "train_labels = train_df[\"label\"].tolist()\n",
        "\n",
        "val_texts = val_df[\"tokens\"].tolist()\n",
        "val_labels = val_df[\"label\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, word_id, embeddings, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.word_id = word_id\n",
        "        self.embeddings = embeddings\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Get indices for tokens in vocab, use 0 if not found\n",
        "        token_ids = [self.word_id.get(token, 0) for token in tokens]\n",
        "\n",
        "        # Pad or truncate to max_len\n",
        "        if len(token_ids) < self.max_len:\n",
        "            token_ids += [0] * (self.max_len - len(token_ids))\n",
        "        else:\n",
        "            token_ids = token_ids[:self.max_len]\n",
        "\n",
        "        token_ids_tensor = torch.tensor(token_ids, dtype=torch.long)\n",
        "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return token_ids_tensor, label_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length: 6554\n",
            "95th percentile length: 1035.5999999999985\n"
          ]
        }
      ],
      "source": [
        "lengths = [len(tokens) for tokens in df['tokens']]\n",
        "print(f\"Max length: {max(lengths)}\")\n",
        "print(f\"95th percentile length: {np.percentile(lengths, 95)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_len = 1000;\n",
        "\n",
        "train_dataset = TextDataset(train_texts, train_labels, vocab, embeddings_np, max_len)\n",
        "val_dataset = TextDataset(val_texts, val_labels, vocab, embeddings_np, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len, :].to(x.device)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.q_linear = torch.nn.Linear(d_model, d_model)\n",
        "        self.k_linear = torch.nn.Linear(d_model, d_model)\n",
        "        self.v_linear = torch.nn.Linear(d_model, d_model)\n",
        "        self.out_linear = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.scale = math.sqrt(self.head_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.q_linear(x)  # (batch_size, seq_len, d_model)\n",
        "        K = self.k_linear(x)\n",
        "        V = self.v_linear(x)\n",
        "\n",
        "        # Split into heads\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # (batch, heads, seq_len, head_dim)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calculate scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # (batch, heads, seq_len, seq_len)\n",
        "        attn_weights = torch.nn.functional.softmax(scores, dim=-1)  # attention weights\n",
        "\n",
        "        out = torch.matmul(attn_weights, V)  # (batch, heads, seq_len, head_dim)\n",
        "\n",
        "        # Concatenate heads\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)  # (batch, seq_len, d_model)\n",
        "\n",
        "        out = self.out_linear(out)  # final linear layer\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dim_feedforward=2048, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.linear1 = torch.nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.linear2 = torch.nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
        "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
        "        self.dropout1 = torch.nn.Dropout(dropout)\n",
        "        self.dropout2 = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Self-attention + Add & Norm\n",
        "        src2 = self.self_attn(src)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # Feedforward + Add & Norm\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "\n",
        "        return src\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerClassifier(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, num_classes, max_seq_len=512):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = PositionalEncoding(embedding_dim, max_seq_len)\n",
        "\n",
        "        self.layers = torch.nn.ModuleList([\n",
        "            TransformerEncoderLayer(embedding_dim, num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.classifier = torch.nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len)\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
        "        x = self.pos_encoder(embedded)  # add positional encoding\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)  # pass through transformer encoder layers\n",
        "\n",
        "        # Pooling: take mean over sequence length\n",
        "        x = x.mean(dim=1)  # (batch_size, embedding_dim)\n",
        "\n",
        "        logits = self.classifier(x)  # (batch_size, num_classes)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Training]: 100%|██████████| 335/335 [09:10<00:00,  1.64s/it, Loss=2.7211]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Final Training Loss: 2.7211\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Validation]: 100%|██████████| 84/84 [00:44<00:00,  1.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Validation Loss: 1.7478, Accuracy: 0.5093, F1 Score: 0.4826\n",
            "Validation loss improved, saving model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Training]: 100%|██████████| 335/335 [09:03<00:00,  1.62s/it, Loss=1.2894]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Final Training Loss: 1.2894\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Validation]: 100%|██████████| 84/84 [00:45<00:00,  1.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Validation Loss: 1.0567, Accuracy: 0.7218, F1 Score: 0.7104\n",
            "Validation loss improved, saving model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Training]: 100%|██████████| 335/335 [10:27<00:00,  1.87s/it, Loss=0.8794]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Final Training Loss: 0.8794\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Validation]: 100%|██████████| 84/84 [00:49<00:00,  1.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Validation Loss: 0.9563, Accuracy: 0.7405, F1 Score: 0.7286\n",
            "Validation loss improved, saving model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Training]: 100%|██████████| 335/335 [10:09<00:00,  1.82s/it, Loss=0.6805]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Final Training Loss: 0.6805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Validation]: 100%|██████████| 84/84 [00:52<00:00,  1.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Validation Loss: 0.8559, Accuracy: 0.7763, F1 Score: 0.7749\n",
            "Validation loss improved, saving model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Training]: 100%|██████████| 335/335 [10:04<00:00,  1.80s/it, Loss=0.5398]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Final Training Loss: 0.5398\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Validation]: 100%|██████████| 84/84 [00:48<00:00,  1.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Validation Loss: 0.8170, Accuracy: 0.7789, F1 Score: 0.7794\n",
            "Validation loss improved, saving model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Training]: 100%|██████████| 335/335 [09:26<00:00,  1.69s/it, Loss=0.4263]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Final Training Loss: 0.4263\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Validation]: 100%|██████████| 84/84 [00:46<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Validation Loss: 0.7684, Accuracy: 0.7954, F1 Score: 0.7953\n",
            "Validation loss improved, saving model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Training]: 100%|██████████| 335/335 [09:30<00:00,  1.70s/it, Loss=0.3245]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7, Final Training Loss: 0.3245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Validation]: 100%|██████████| 84/84 [00:51<00:00,  1.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7, Validation Loss: 0.8380, Accuracy: 0.7857, F1 Score: 0.7872\n",
            "No improvement in validation loss for 1 epoch(s).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Training]: 100%|██████████| 335/335 [09:27<00:00,  1.69s/it, Loss=0.2467]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, Final Training Loss: 0.2467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Validation]: 100%|██████████| 84/84 [00:45<00:00,  1.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, Validation Loss: 0.8658, Accuracy: 0.7984, F1 Score: 0.8006\n",
            "No improvement in validation loss for 2 epoch(s).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Training]: 100%|██████████| 335/335 [08:59<00:00,  1.61s/it, Loss=0.1805]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9, Final Training Loss: 0.1805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Validation]: 100%|██████████| 84/84 [00:44<00:00,  1.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9, Validation Loss: 0.9551, Accuracy: 0.7797, F1 Score: 0.7790\n",
            "No improvement in validation loss for 3 epoch(s).\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=100,\n",
        "    num_heads=4,\n",
        "    num_layers=1,\n",
        "    num_classes=43,\n",
        "    max_seq_len=1000\n",
        ").to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience = 3         # how many epochs to wait\n",
        "counter = 0          # how many bad epochs seen in a row\n",
        "delta = 0.01         # minimum improvement to reset patience\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\")\n",
        "    for inputs, targets in train_loop:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        avg_loss = total_loss / (train_loop.n + 1)\n",
        "        train_loop.set_postfix({'Loss': f'{avg_loss:.4f}'})\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Final Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_total_loss = 0\n",
        "    all_targets = []\n",
        "    all_preds = []\n",
        "\n",
        "    val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\")\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loop:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs, targets)\n",
        "            val_total_loss += val_loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_total_loss / len(val_loader)\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}, Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # ---- Early Stopping Check ----\n",
        "    if best_val_loss - avg_val_loss > delta:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')  # save the best model\n",
        "        print(\"Validation loss improved, saving model.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"No improvement in validation loss for {counter} epoch(s).\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'trained_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
